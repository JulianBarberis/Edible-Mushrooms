{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura de la Red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"Red_Neuronal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en el grafico anterior, la red tendra **2 capas ocultas y 1 de salida**.\n",
    "\n",
    "En la **primer capa oculta** contaremos con **8 neuronas**, en la **segunda capa oculta** tendremos **4 neuronas** y en la **capa de salida tendremos 1 neurona** que nos indicara si el hongo es **comestible o no**.\n",
    "\n",
    "Para las **capas ocultas** utilizaremos la **función de activación Relu** ya que convierte los valores negativos en 0 y mitiga el proble,a del gradiente desvaneciente.\n",
    "Por otro lado, para la **capa de salida** utilizaremos la **función de activación Logística** ya que devuelve valores entre 0 y 1 y eso es ideal para nuestra red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacion en numpy de la Red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "all_data = pd.read_csv(\"mushrooms_limpios.csv\") #22418 (Cantidad de registros)\n",
    "\n",
    "all_inputs = all_data.iloc[:, 1:].values # Selecciono todas las columnas de entrada del conjunto de datos, menos la primera que es la de salida\n",
    "all_outputs = all_data.iloc[:, 0].values # Selecciono la columna de salida del conjunto de datos\n",
    "\n",
    "scaler = StandardScaler() # Escalo los datos de entrada \n",
    "all_inputs = scaler.fit_transform(all_inputs) # Transformo las entradas para tener una media de 0 y una desviación estándar de 1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(all_inputs, all_outputs, test_size=1 / 3) # Divido los conjuntos de datos de entrenamiento y prueba\n",
    "n = X_train.shape[0] \n",
    "\n",
    "# Defino funciones de activación\n",
    "relu = lambda x: np.maximum(x, 0)  # ReLU porque no tengo valores negativos\n",
    "logistic = lambda x: 1 / (1 + np.exp(-x)) # Logística para la capa de salida (salida binaria) \n",
    "\n",
    "np.random.seed(22) # Inicializo semilla en 22 para reproducibilidad\n",
    "\n",
    "w_hidden = np.random.rand(8, 11)   # Pesos primera capa oculta\n",
    "w_hidden2 = np.random.rand(4, 8)   # Pesos segunda capa oculta        \n",
    "w_output = np.random.rand(1, 4)    # Pesos capa de salida\n",
    "\n",
    "b_hidden = np.random.rand(8, 1)    # Sesgos primera capa oculta   \n",
    "b_hidden2 = np.random.rand(4, 1)   # Sesgos segunda capa oculta\n",
    "b_output = np.random.rand(1, 1)    # Sesgos capa de salida\n",
    "\n",
    "# Función de forward propagation\n",
    "def forward_prop(X): \n",
    "    Z1 = w_hidden @ X + b_hidden       # Entrada ponderada de la primera capa oculta\n",
    "    A1 = relu(Z1)                      # ReLU en la primera capa oculta\n",
    "    Z2 = w_hidden2 @ A1 + b_hidden2    # Entrada ponderada de la segunda capa oculta\n",
    "    A2 = relu(Z2)                      # ReLU en la segunda capa oculta\n",
    "    Z3 = w_output @ A2 + b_output      # Entrada ponderada de la capa de salida\n",
    "    A3 = logistic(Z3)                  # Logistica en la capa de salida\n",
    "    return Z1, A1, Z2, A2, Z3, A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisión No entrenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5329854141576078\n"
     ]
    }
   ],
   "source": [
    "test_predictions = forward_prop(X_test.transpose())[5]  # Consulto A3 (Capa de salida)\n",
    "test_comparisons = np.equal((test_predictions >= .5).flatten().astype(int), Y_test) # Comparo valores predichos con reales\n",
    "accuracy = sum(test_comparisons.astype(int) / X_test.shape[0]) # Calculo la precisión\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y Evaluacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.05 # Tasa de aprendizaje\n",
    "\n",
    "# Derivadas de las funciones de activación\n",
    "d_relu = lambda x: x > 0\n",
    "d_logistic = lambda x: np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "# Función de backward propagation\n",
    "def backward_prop(Z1, A1, Z2, A2, Z3, A3, X, Y): \n",
    "    # Calculo de las derivadas para obtener la derivada del costo con respecto a mis pesos y sesgos (W3, W2, W1, B3, B2 B1)\n",
    "    dC_dA3 = 2 * (A3 - Y)           \n",
    "    dA3_dZ3 = d_logistic(Z3)\n",
    "    dZ3_dW3 = A2\n",
    "    dZ3_dA2 = w_output\n",
    "    dC_dZ3 = dC_dA3 * dA3_dZ3   \n",
    "\n",
    "    dC_dA2 = dZ3_dA2.T @ dC_dZ3\n",
    "    dA2_dZ2 = d_relu(Z2)\n",
    "    dZ2_dW2 = A1\n",
    "    dZ2_dA1 = w_hidden2\n",
    "    dC_dZ2 = dC_dA2 * dA2_dZ2\n",
    "\n",
    "    dC_dA1 = dZ2_dA1.T @ dC_dZ2\n",
    "    dA1_dZ1 = d_relu(Z1)\n",
    "    dZ1_dW1 = X\n",
    "    dC_dZ1 = dC_dA1 * dA1_dZ1\n",
    "\n",
    "    # Calculo los gradientes de la función de costo con respecto a los pesos y sesgos de cada capa.\n",
    "    # keepdims sirve para mantener las dimensiones de entrada\n",
    "    dC_dW3 = dC_dZ3 @ dZ3_dW3.T\n",
    "    dC_dB3 = np.sum(dC_dZ3, axis=1, keepdims=True)  \n",
    "\n",
    "    dC_dW2 = dC_dZ2 @ dZ2_dW2.T\n",
    "    dC_dB2 = np.sum(dC_dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dC_dW1 = dC_dZ1 @ dZ1_dW1.T\n",
    "    dC_dB1 = np.sum(dC_dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dC_dW1, dC_dB1, dC_dW2, dC_dB2, dC_dW3, dC_dB3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descenso de Gradiente Estocástico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(120_000):\n",
    "    # Selecciono aleatoriamente uno de los datos de entrenamiento\n",
    "    idx = np.random.choice(n, 1, replace=False)\n",
    "    X_sample = X_train[idx].transpose()\n",
    "    Y_sample = Y_train[idx]\n",
    "\n",
    "    # Los paso aleatoriamente por la red neuronal\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_prop(X_sample)\n",
    "\n",
    "    # Realizo retropropagación y devuelvo los pesos y sesgos\n",
    "    dW1, dB1, dW2, dB2, dW3, dB3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, X_sample, Y_sample)\n",
    "\n",
    "    # Actualizo pesos y sesgos con la taza de aprendizaje \n",
    "    w_hidden -= L * dW1\n",
    "    b_hidden -= L * dB1\n",
    "    w_hidden2 -= L * dW2\n",
    "    b_hidden2 -= L * dB2\n",
    "    w_output -= L * dW3\n",
    "    b_output -= L * dB3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisión Red Entrenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8966878554700569\n",
      "Test Accuracy: 0.8837147062759266\n"
     ]
    }
   ],
   "source": [
    "# Calculo de precisión de entrenamiento\n",
    "train_predictions = forward_prop(X_train.transpose())[5]  # Consulto A3 (Capa de salida)\n",
    "train_comparisons = np.equal((train_predictions >= .5).flatten().astype(int), Y_train) # Comparo valores predichos con reales\n",
    "train_accuracy = sum(train_comparisons.astype(int)) / X_train.shape[0] # Calculo la precisión de entrenamiento\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "    \n",
    "    \n",
    "# Calculo de precisión de prueba\n",
    "test_predictions = forward_prop(X_test.transpose())[5]  # Consulto A3 (Capa de salida)\n",
    "test_comparisons = np.equal((test_predictions >= .5).flatten().astype(int), Y_test) # Comparo valores predichos con reales\n",
    "test_accuracy = sum(test_comparisons.astype(int)) / X_test.shape[0] # Calculo la precisión de prueba\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"mi_red_iteraciones=150000_L=0.05.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en el grafico anterior, cuando alcanzamos las 120000 iteraciones aproximadamente, conseguimos la menor variacion del resultado y la menor diferencia entre test y train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación con scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntaje de entrenamiento: 0.891134\n",
      "Puntaje de prueba: 0.894955\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "df = pd.read_csv(\"mushrooms_limpios.csv\")\n",
    "\n",
    " \n",
    "X = df.values[:, 1:] # Selecciono todas las columnas de entrada del conjunto de datos, menos la primera que es la de salida\n",
    "scaler = StandardScaler() # Escalo los datos de entrada\n",
    "X  = scaler.fit_transform(X)\n",
    "\n",
    "Y = df.values[:, 0] # Selecciono la columna de salida del conjunto de datos\n",
    "\n",
    "# Separar los datos de entrenamiento y prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3)\n",
    "\n",
    "nn = MLPClassifier(solver='adam',\n",
    "                   hidden_layer_sizes=(8, 4, ),\n",
    "                   activation='relu',\n",
    "                   max_iter=120_000,\n",
    "                   random_state=22,\n",
    "                   learning_rate_init=.05)\n",
    "\n",
    "nn.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Puntaje de entrenamiento: %f\" % nn.score(X_train, Y_train))\n",
    "print(\"Puntaje de prueba: %f\" % nn.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados son casi iguales, estando un poco por encima los resultados de la libreria, que es normal debido a que tiene una mejor y mas óptima implementación.\n",
    "\n",
    "Claramente notamos diferencia sobre todo en el tiempo de ejecucion ya que mi red tarda casi 1 minuto en ejecutar el descenso de gradiente estocastico y la de scikit-learn tarda menos de 2 segundos. Esto se debe a que la libreria scikit-learn utiliza metodos de optimizacion que le permiten ejecutarse mucho mas rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión Final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar una red neuronal para clasificación binaria desde cero me ha proporcionado una comprensión mas profunda del funcionamiento de las mismas. \n",
    "\n",
    "A lo largo del proyecto, he aprendido a seleccionar un Data Frame, a entenderlo, a manipularlo de manera que me sirva para desarrollar mi red. Ademas, aprendí a limpiarlo de valores atipicos y vacíos, y a elegir cuando esos valores me sirven o me perjudican.\n",
    "\n",
    "Luego, aprendi a desarrollar paso a paso mi red a lo largo del desarrollo, cambiando la cantidad de iteraciones, la taza de aprendizaje y analizando sus graficos para elegir los mejores valores posibles.\n",
    "\n",
    "Al comparar esta experiencia con el uso de librerías como scikit-learn, es evidente que construir una red neuronal desde cero ofrece una mayor comprensión del funcionamiento de la misma y una mayor personalización. Sin embargo, el tiempo de desarrollo y la probabilidad de errores son mucho mayores. \n",
    "Teniendo esto en cuenta, usar la libreria, es mucho mas práctico para cuando la necesitamos para un proyecto simple o rápido y que no merezca mucho analisis, ademas, con la libreria podemos comparar distintos metodos de aprendizaje que pueden implementarse ante el mismo problema.\n",
    "\n",
    "En conclusión, mi opinión es que desarrollar tu propia Red te ayuda mucho mas a entender realmente su funcionamiento, su poder y sus limitaciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
